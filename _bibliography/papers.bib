---
---

@article{alivanistos2022prompting,
  abbr = {Competition winner},
  title = {Prompting as Probing: {{Using}} Language Models for Knowledge Base Construction},
  author = {Alivanistos, Dimitrios and Báez Santamarı́a, Selene and Cochez, Michael and Kalo, Jan Christoph and van Krieken, Emile and Thanapalasingam, T and others},
  date = {2022},
  publisher = {AachenCEUR-WS}
}

@inproceedings{danieleRefiningNeuralNetwork2023a,
  abbr = {ML Journal},
  title = {Refining Neural Network Predictions Using Background Knowledge},
  author = {Daniele, Alessandro and family=Krieken, given=Emile, prefix=van, useprefix=true and Serafini, Luciano and family=Harmelen, given=Frank, prefix=van, useprefix=true},
  date = {2023-03-14},
  journaltitle = {Machine Learning},
  shortjournal = {Mach Learn},
  issn = {1573-0565},
  doi = {10.1007/s10994-023-06310-3},
  url = {https://doi.org/10.1007/s10994-023-06310-3},
  urldate = {2023-03-29},
  abstract = {Recent work has shown learning systems can use logical background knowledge to compensate for a lack of labeled training data. Many methods work by creating a loss function that encodes this knowledge. However, often the logic is discarded after training, even if it is still helpful at test time. Instead, we ensure neural network predictions satisfy the knowledge by refining the predictions with an extra computation step. We introduce differentiable refinement functions that find a corrected prediction close to the original prediction. We study how to effectively and efficiently compute these refinement functions. Using a new algorithm called iterative local refinement (ILR), we combine refinement functions to find refined predictions for logical formulas of any complexity. ILR finds refinements on complex SAT formulas in significantly fewer iterations and frequently finds solutions where gradient descent can not. Finally, ILR produces competitive results in the MNIST addition task.},
  langid = {english},
  keywords = {Fuzzy logic,Neurosymbolic AI,Optimization},
  arxiv = {2206.04976}
}

@inproceedings{heinerman2018benefits,
  abbr = {IEEE SSCI},
  title = {Benefits of Social Learning in Physical Robots},
  booktitle = {2018 {{IEEE}} Symposium Series on Computational Intelligence ({{SSCI}})},
  author = {Heinerman, Jacqueline and Bussmann, Bart and Groenendijk, Rick and Van Krieken, Emile and Slik, Jesper and Tezza, Alessandro and Haasdijk, Evert and family=Eiben, given=AE, given-i=AE},
  date = {2018},
  pages = {851--858},
  publisher = {IEEE}
}

@article{PhysRev.47.777,
  abbr={PhysRev},
  title={Can Quantum-Mechanical Description of Physical Reality Be Considered Complete?},
  author={Einstein*†, A. and Podolsky*, B. and Rosen*, N.},
  abstract={In a complete theory there is an element corresponding to each element of reality. A sufficient condition for the reality of a physical quantity is the possibility of predicting it with certainty, without disturbing the system. In quantum mechanics in the case of two physical quantities described by non-commuting operators, the knowledge of one precludes the knowledge of the other. Then either (1) the description of reality given by the wave function in quantum mechanics is not complete or (2) these two quantities cannot have simultaneous reality. Consideration of the problem of making predictions concerning a system on the basis of measurements made on another system that had previously interacted with it leads to the result that if (1) is false then (2) is also false. One is thus led to conclude that the description of reality as given by a wave function is not complete.},
  journal={Phys. Rev.},
  location={New Jersey},
  volume={47},
  issue={10},
  pages={777--780},
  numpages={0},
  year={1935},
  month={May},
  publisher=aps,
  doi={10.1103/PhysRev.47.777},
  url={http://link.aps.org/doi/10.1103/PhysRev.47.777},
  html={https://journals.aps.org/pr/abstract/10.1103/PhysRev.47.777},
  pdf={example_pdf.pdf},
  altmetric={248277},
  dimensions={true},
  google_scholar_id={qyhmnyLat1gC},
  video={https://www.youtube-nocookie.com/embed/aqz-KE-bpKQ},
  additional_info={. *More Information* can be [found here](https://github.com/alshedivat/al-folio/)},
  annotation={* Example use of superscripts<br>† Albert Einstein},
  selected={true},
  inspirehep_id = {3255}
}

@inproceedings{NEURIPS2023_4d9944ab,
  abbr = {NeurIPS},
  title = {A-{{NeSI}}: {{A}} Scalable Approximate Method for Probabilistic Neurosymbolic Inference},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {family=Krieken, given=Emile, prefix=van, useprefix=true and Thanapalasingam, Thiviyan and Tomczak, Jakub and family=Harmelen, given=Frank, prefix=van, useprefix=true and Ten Teije, Annette},
  editor = {Oh, A. and Neumann, T. and Globerson, A. and Saenko, K. and Hardt, M. and Levine, S.},
  date = {2023},
  volume = {36},
  pages = {24586--24609},
  publisher = {Curran Associates, Inc.},
  url = {https://proceedings.neurips.cc/paper_files/paper/2023/file/4d9944ab3330fe6af8efb9260aa9f307-Paper-Conference.pdf},
  abstract = {We study the problem of combining neural networks with symbolic reasoning. Recently introduced frameworks for Probabilistic Neurosymbolic Learning (PNL), such as DeepProbLog, perform exponential-time exact inference, limiting the scalability of PNL solutions. We introduce Approximate Neurosymbolic Inference (A-NeSI): a new framework for PNL that uses neural networks for scalable approximate inference. A-NeSI 1) performs approximate inference in polynomial time without changing the semantics of probabilistic logics; 2) is trained using data generated by the background knowledge; 3) can generate symbolic explanations of predictions; and 4) can guarantee the satisfaction of logical constraints at test time, which is vital in safety-critical applications. Our experiments show that A-NeSI is the first end-to-end method to solve three neurosymbolic tasks with exponential combinatorial scaling. Finally, our experiments show that A-NeSI achieves explainability and safety without a penalty in performance.},
  arxiv = {2212.12393},
  selected = {true},
  code = {https://github.com/HEmile/a-nesi}
}

